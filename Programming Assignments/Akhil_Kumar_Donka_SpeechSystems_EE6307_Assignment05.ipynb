{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVmT47LOBy6i"
      },
      "source": [
        "<center> <h1> <b> Speech Systems (EE6307) </b> </h1> </center>\n",
        "\n",
        "<b> Programming Assignment - 05 - Statistical and Deep Learning approaches to Automatic Speaker Verification </b>\n",
        "\n",
        "Welcome to the fifth programming assignment in the speech systems (EE6307) course. The current programming assignment expects you to automate the speaker verification system (ASV). Automatic speaker verification is the task of verifying the claimed identity of a speaker based on their voice characteristics. In simpler words, Given two utterances, The ASV system should output a binary value saying whether they came from the same speaker or not. The current programming assignment is broadly classified into two parts. In the first part of the programming assignment, you need to code an algorithm to automate the speaker verification system using GMM-UBM models. The second part of the programming assignment requires you to design discriminative methods ( X-vector ) for the ASV system. Compare the generative ( GMM-UBM) with discriminative ( X-vector ) models for ASV, and write down your observations. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<b> Instructions </b>\n",
        "1. Plagiarism is strictly prohibited.\n",
        "2. Delayed submissions will be penalized with a scaling factor of 0.5 per day.\n",
        "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reXi8STg3pM5"
      },
      "source": [
        "<h4> <b> Statistical approaches to speaker verification (GMM-UBM models) : </b>  This section describes the steps required to build an ASV system using GMM-UBM models. As the primary goal of the programming assignment is to build an ASV system and understand its pros and cons, We request you use either your previous assignment codes or built-in codes from python packages to extract the features. You can also use the scikit-learn package to estimate the parameters of GMM models. Please download the data from the below link and go through the Readme file for finer details i.e, data for training the UBM model, data for evaluating the ASV system  </h4> \n",
        "\n",
        "[link to data](https://drive.google.com/drive/folders/1Btnpm_QwSirInGKvL0yq5UL_zL23rM1f?usp=sharing)\n",
        "\n",
        "<dt> <h4> 1. Feature Extraction </h4> </dt> \n",
        "<dd> <h4> - Extract 30-dimensional Mel Frequency Cepstral Coefficients (MFCCs) from the speech signal. Add delta (velocity) and delta-delta (acceleration) coefficients to include the formant dynamics in the features. Use the following parameters for extracting the features. Framesize = 25 msec, Frameshift = 10 msec and number of mel filter banks = 40  </h4> </dd> \n",
        "<dt> <h4> 2. Feature preprocessing </h4> </dt> \n",
        "<dd> <h4> - Process the features using cepstral mean-variance normalization (CMVN) to remove the convolutive noise. Use the optimal window in CMVN preprocessing. Typically it is preferred to use 3 seconds.   </h4> </dd> \n",
        "<dd> <h4> - Voice Activity Detector (VAD): Silence regions in the speech signal will not contain speaker information, and including them in GMM modeling will deteriorate the performance. Hence, remove the silence/noise frames using an energy-based voice activity detector module  </h4> </dd> \n",
        "<dt> <h4> 2. Universal Background Modeling (UBM) </h4> </dt> \n",
        "<dd> <h4> - Let us consider a Gaussian Mixture Model (GMM) with 128 Gaussians.  </h4> </dd> \n",
        "<dd> <h4> - Estimate the parameters of the GMM model from large number of background speakers. Make sure that the background speakers are not involved in testing.   </h4> </dd> \n",
        "<dd> <h4> - Speech data from 100 speakers are provided to estimate the UBM parameters. If you do not have sufficient computational resources, please use less number of speakers to estimate the parameters.   </h4> </dd>\n",
        "<dt> <h4> 2. Speaker-specific GMM's </h4> </dt> \n",
        "<dd> <h4> - Inputs : Trained UBM model i.e $\\lambda^{ubm}$ and speaker training data i.e X   </h4> </dd> \n",
        "<dd> <h4> - Stats estimation from UBM model: Compute the responsibility of the Gaussians with the initial estimates as UBM parameters </h4> </dd> \n",
        "<dd> <h4> <center> $\\gamma_{nk}$ = $\\frac{\\pi_{k}^{ubm} N ( x_{n} / \\mu_{k}^{ubm} , \\sigma_{k}^{2} )}{ \\sum_{m=1}^{M} \\pi_{m}^{ubm} N ( x_{n} / \\mu_{m}^{ubm} , \\sigma_{m}^{2} ) }$ </center> </h4> </dd> \n",
        "<dd> <h4> where $\\gamma_{nk}$ represents the responsibility of $k^{th}$ gaussian in generating $n^{th}$ data point and {$ \\pi_{m}^{ubm} , \\mu_{m}^{ubm} , \\sigma_{m}^{2} $} are the UBM parameters    </h4> </dd> \n",
        "\n",
        "<dd> <h4> - EM update for $\\mu_{k}$ and  $\\pi_{k}$: Estimate the parameters of the GMM { $\\mu_{k}$,$\\pi_{k}$ } using the available speaker's data  </h4> </dd> \n",
        "<dd> <h4> <center> $\\mu_{k}^{em}$ = $\\frac{1}{N_{k}}$ $\\sum_{n=1}^{N}$ $\\gamma_{nk}x_{n}$ </center> \n",
        "<center> $\\pi_{k}^{em}$ = $\\frac{\\sum\\limits_{n=1}^{N} \\gamma_{nk}}{N} $ </center>  </h4> </dd> \n",
        "\n",
        "<dd> <h4> - MAP update : Considering the UBM model as the prior information, The MAP adaption results in the following updates to zeroth order{$\\pi_{k}$} and first-order statistics ($\\mu_{k}$)   </h4> </dd> \n",
        "<dd> <h4> <center> $\\mu_{k}^{spk}$ = $\\alpha\\mu_{k}^{ubm}$ + $(1-\\alpha)\\mu_{k}^{em}$ </center> <br>\n",
        "<center> $\\pi_{k}^{spk}$ = $\\alpha\\pi_{k}^{ubm}$ + $(1-\\alpha)\\pi_{k}^{em}$ </center>   </h4> </dd> \n",
        "<dd> <h4>  Where $\\alpha$ is given by $\\frac{N_{k}}{N_{k} + r}$ and \"r\" is the relevance factor.  </h4> </dd> \n",
        "\n",
        "<dt> <h4> 3. Compute the likelihood ratio </h4> </dt> \n",
        "<dd> <h4> - Estimate the score for the hypothesis that the given speaker came from the hypothesized speaker  </h4> </dd>\n",
        "<dd> <h4> - Let X be the set of feature vectors from the test utterance  </h4> </dd>\n",
        "<dd> <h4> - Let k be the claimed identity of the speaker. \n",
        "<dd> <h4> - H0 = Feature set X is from speaker k  </h4> </dd>\n",
        "<dd> <h4> - H1 = Feature set X is not from speaker k   </h4> </dd>\n",
        "<dd> <h4> - Hypothesis testing based on likelihood ratio   </h4> </dd>\n",
        "\n",
        "<dd> <h4>\n",
        "</br>\n",
        "<center>  $\\frac{P(X/\\lambda_{k})}{P(X/\\lambda_{\\bar{k}})}$ >= $\\theta$ accepts H0 </center> </br>\n",
        "<center>  $\\frac{P(X/\\lambda_{k})}{P(X/\\lambda_{\\bar{k}})}$ < $\\theta$ reject H0 </center>  <br>\n",
        "</h4> </dd> \n",
        "\n",
        "<dd> <h4> $\\lambda_{k}$ denotes model estimated from reference utterance of k. </h4> </dd>\n",
        "<dd> <h4> $\\lambda_{\\bar{k}}$ denotes model estimated from entire world except k. Let us consider UBM model here. </h4> </dd> </h4> </dd>\n",
        "\n",
        "<dt> <h4> 4. Performance Evaluation :  </h4> </dt> \n",
        "\n",
        "<dd> <h4> - Speech data from 40 speakers are provided to evaluate the ASV performance </h4> </dd>\n",
        "<dd> <h4> - Compute Equal Error Rate (EER) to evaluate the developed ASV system.  </h4> </dd>\n",
        "<dd> <h4> - Please do evaluate the EER on the set of verification trials ( 1000 trials ) and report it  </h4> </dd>\n",
        "\n",
        "<dt> <h4> 5. Optinal :  </h4> </dt> \n",
        "\n",
        "<dd> <h4> - Experiment with varying amount of speech data in UBM modeling </h4> </dd>\n",
        "<dd> <h4> - Does it depends on the number of speakers in the UBM data or the amount of speech in it ? </h4> </dd>\n",
        "<dd> <h4> - Report your observations </h4> </dd>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpu2gRlNB631",
        "outputId": "6e8b8025-78cf-40ec-833f-dd67c144fa12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path='/content/drive/My Drive/speechAudio/ASV_Data.zip'\n",
        "zip_ref = zipfile.ZipFile(zip_path, 'r')\n",
        "# zip_ref.extractall(\"/content/drive/My Drive/speechAudio/\")\n",
        "zip_ref.extractall(\"/content/\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEvKdu70FqoK",
        "outputId": "fc70bdf3-d5ba-424c-e32b-0f87e5ee6f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting speechpy\n",
            "  Downloading speechpy-2.4-py2.py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from speechpy) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from speechpy) (1.7.3)\n",
            "Installing collected packages: speechpy\n",
            "Successfully installed speechpy-2.4\n"
          ]
        }
      ],
      "source": [
        "#################################\n",
        "#Import all the modules\n",
        "#################################\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import random as rand\n",
        "from scipy.stats import norm, multivariate_normal\n",
        "import sys\n",
        "import math\n",
        "\n",
        "!pip install speechpy\n",
        "import speechpy\n",
        "from tqdm import tqdm "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "#Dataset Manifests using Pandas\n",
        "#################################\n",
        "\n",
        "data_path='/content/ASV_Data/UBM_data'\n",
        "\n",
        "dataframe = []\n",
        "\n",
        "for subdir, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        #print os.path.join(subdir, file)\n",
        "        filepath = subdir + os.sep + file\n",
        "\n",
        "        if filepath.endswith(\".wav\"):\n",
        "\n",
        "            data = {\n",
        "                'filepath'  : filepath,\n",
        "                'speaker'   : os.path.basename(os.path.dirname(subdir))\n",
        "            }\n",
        "\n",
        "            dataframe.append(data)\n",
        "\n",
        "dfs = pd.DataFrame(dataframe)\n",
        "dfs.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RlWBjNsGJ0Nn",
        "outputId": "7dcdefcb-43c9-4552-9de3-be1dc17d6117"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            filepath  speaker\n",
              "0  /content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...  id10855\n",
              "1  /content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...  id10855\n",
              "2  /content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...  id10855\n",
              "3  /content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...  id10855\n",
              "4  /content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...  id10855"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9ace729-8a0f-423f-abe3-594a09ef0ec9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>speaker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...</td>\n",
              "      <td>id10855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...</td>\n",
              "      <td>id10855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...</td>\n",
              "      <td>id10855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...</td>\n",
              "      <td>id10855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10855/_MCGF6XIzfI...</td>\n",
              "      <td>id10855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9ace729-8a0f-423f-abe3-594a09ef0ec9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d9ace729-8a0f-423f-abe3-594a09ef0ec9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d9ace729-8a0f-423f-abe3-594a09ef0ec9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting 20 Unique Speakers for UBM for less computation\n",
        "UBM_Speakers = dfs.speaker.unique()[:20]"
      ],
      "metadata": {
        "id": "mgqi6ounKZOo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "# Feature Extraction for UBM Model\n",
        "#################################\n",
        "\n",
        "speaker_dfs = {}\n",
        "for speaker in UBM_Speakers:\n",
        "  speaker_dfs[speaker] = dfs.loc[dfs['speaker'] == speaker]\n",
        "\n",
        "speaker_feats = {}\n",
        "count = 0\n",
        "for speaker in UBM_Speakers:\n",
        "  print(\"Extracting Features for Speaker \" + str(count+1) + \"/20\")\n",
        "  df = speaker_dfs[speaker]\n",
        "  stack = []\n",
        "  for ind in tqdm(df.index):\n",
        "    y, sr = librosa.load(df[\"filepath\"][ind], sr = None)\n",
        "    y = y/max(abs(y))\n",
        "    hop_length = int(sr*0.010)\n",
        "    frameSize = int(sr*0.025)\n",
        "    energies = librosa.feature.rms(y=y, frame_length = frameSize, hop_length = hop_length)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=20, win_length=frameSize)\n",
        "    indexes = energies[0] > 0.1\n",
        "    if len(indexes) == mfcc.shape[1]:\n",
        "      mfcc_delta = librosa.feature.delta(mfcc)\n",
        "      mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "      feature = np.vstack([mfcc, mfcc_delta, mfcc_delta2])\n",
        "      feature = speechpy.processing.cmvnw(feature, win_size=301, variance_normalization=True)\n",
        "      stack.append(feature[:,indexes])\n",
        "  speaker_feats[speaker] = stack\n",
        "  count += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbgrT1pUKkqO",
        "outputId": "9a629bd8-d5f5-4573-dd24-70614726be5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 124/124 [00:18<00:00,  6.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 82/82 [00:12<00:00,  6.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:10<00:00,  4.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [00:09<00:00,  7.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 141/141 [00:18<00:00,  7.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [00:28<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:08<00:00,  7.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 185/185 [00:19<00:00,  9.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 65/65 [00:08<00:00,  7.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 112/112 [00:13<00:00,  8.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59/59 [00:09<00:00,  6.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 53/53 [00:06<00:00,  8.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 220/220 [00:24<00:00,  8.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54/54 [00:06<00:00,  7.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 197/197 [00:25<00:00,  7.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:06<00:00,  8.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 82/82 [00:12<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:10<00:00,  7.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 113/113 [00:14<00:00,  7.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 198/198 [00:27<00:00,  7.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "#Building UBM Model\n",
        "#################################\n",
        "\n",
        "# stacking all features for single speaker =========> (60, T*Wavefiles)\n",
        "speaker_wise_stacked = {}\n",
        "\n",
        "for speaker in speaker_feats.keys():\n",
        "  speaker_wise_stacked[speaker] = np.concatenate(speaker_feats[speaker], axis=1)\n",
        "\n",
        "# stacking all features for all speakers =========> (60, T*Wavefiles*speakers)\n",
        "X = np.concatenate(list(speaker_wise_stacked.values()), axis=1)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tjGoMoXLh2i",
        "outputId": "1adb5a19-0f76-40a4-8525-762b0dc52118"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60, 728784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UBM Model Training\n",
        "\n",
        "ubm_gm = GaussianMixture(n_components=128, random_state=0, covariance_type=\"diag\").fit(X.T)"
      ],
      "metadata": {
        "id": "vSHVllafQK7m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ubm_gm.covariances_.shape"
      ],
      "metadata": {
        "id": "65aaqpjyezUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564366e6-c7a5-4c94-f65b-3425bca09723"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 60)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "#Building Speaker Models\n",
        "#################################\n",
        "\n",
        "data_path='/content/ASV_Data/evaluation_data'\n",
        "\n",
        "dataframe = []\n",
        "\n",
        "for subdir, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        #print os.path.join(subdir, file)\n",
        "        filepath = subdir + os.sep + file\n",
        "        y, sr = librosa.load(filepath, sr = None)\n",
        "\n",
        "        if filepath.endswith(\".wav\"):\n",
        "\n",
        "            data = {\n",
        "                'filepath'  : filepath,\n",
        "                'speaker'   : os.path.basename(os.path.dirname(subdir))\n",
        "            }\n",
        "\n",
        "            dataframe.append(data)\n",
        "\n",
        "dfs = pd.DataFrame(dataframe)\n",
        "\n",
        "# Selecting 5 Unique Speakers for less computation\n",
        "speakers = dfs.speaker.unique()[:5]\n",
        "\n",
        "print(\"Selected Speakers : \", speakers)\n",
        "\n",
        "speaker_dfs = {}\n",
        "for speaker in speakers:\n",
        "  speaker_dfs[speaker] = dfs.loc[dfs['speaker'] == speaker]\n",
        "\n",
        "print(\"Created Manifests and Selected 5 Unique Speakers\")\n",
        "\n",
        "# Extracting features\n",
        "speaker_feats = {}\n",
        "count = 0\n",
        "for speaker in speakers:\n",
        "  print(\"Extracting Features for Speaker \" + str(count+1) + \"/5\")\n",
        "  df = speaker_dfs[speaker].head(1)\n",
        "  stack = []\n",
        "  for ind in tqdm(df.index):\n",
        "    y, sr = librosa.load(df[\"filepath\"][ind], sr = None)\n",
        "    y = y/max(abs(y))\n",
        "    hop_length = int(sr*0.010)\n",
        "    frameSize = int(sr*0.025)\n",
        "    energies = librosa.feature.rms(y=y, frame_length = frameSize, hop_length = hop_length)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=20, win_length=frameSize)\n",
        "    indexes = energies[0] > 0.1\n",
        "    if len(indexes) == mfcc.shape[1]:\n",
        "      mfcc_delta = librosa.feature.delta(mfcc)\n",
        "      mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "      feature = np.vstack([mfcc, mfcc_delta, mfcc_delta2])\n",
        "      feature = speechpy.processing.cmvnw(feature, win_size=301, variance_normalization=True)\n",
        "      stack.append(feature[:,indexes])\n",
        "  speaker_feats[speaker] = stack\n",
        "  count += 1\n",
        "\n",
        "speaker_wise_stacked = {}\n",
        "\n",
        "for speaker in speaker_feats.keys():\n",
        "  speaker_wise_stacked[speaker] = np.concatenate(speaker_feats[speaker], axis=1).T\n",
        "\n",
        "print(\"Created Stacked features for all selected speakers\")"
      ],
      "metadata": {
        "id": "TqVjkcnRRdn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379ab7bf-eb57-4e07-a036-2569ad97829b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Speakers :  ['id10287' 'id10303' 'id10286' 'id10307' 'id10296']\n",
            "Created Manifests and Selected 5 Unique Speakers\n",
            "Extracting Features for Speaker 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 13.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Features for Speaker 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created Stacked features for all selected speakers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating GMM Models for each speaker\n",
        "\n",
        "epsilon = 1e-6*np.identity(60)\n",
        "\n",
        "def initialization(X, ubm_gm, M = 128):\n",
        "  mus = ubm_gm.means_\n",
        "  covs = np.zeros((M,len(X[0]),len(X[0])))\n",
        "  for g in range(len(covs)):\n",
        "    covs[g] = np.diag(ubm_gm.covariances_[g])\n",
        "  pi = ubm_gm.weights_\n",
        "  theta = {\"mus\" : mus, \"covs\" : covs, \"pi\" : pi}\n",
        "  return theta\n",
        "\n",
        "############################################################################\n",
        "#Write a function to compute the log-likelihood of the data given the parameters\n",
        "############################################################################\n",
        "\n",
        "def logLikelihood(X, theta, M = 128):\n",
        "  pi =  theta[\"pi\"]\n",
        "  mus =  theta[\"mus\"]\n",
        "  covs =  theta[\"covs\"]\n",
        "  J = np.log(np.sum([k*multivariate_normal(mus[i],covs[j]).pdf(X) for k,i,j in zip(pi,range(len(mus)),range(len(covs)))]))\n",
        "  return J\n",
        "\n",
        "def likelihood(X, theta, M = 128):\n",
        "  pi =  theta[\"pi\"]\n",
        "  mus =  theta[\"mus\"]\n",
        "  covs =  theta[\"covs\"]\n",
        "  J = np.sum([k*multivariate_normal(mus[i],covs[j]).pdf(X) for k,i,j in zip(pi,range(len(mus)),range(len(covs)))])\n",
        "  return J\n",
        "\n",
        "############################################################################\n",
        "#Write a function for the expectation step \n",
        "############################################################################\n",
        "\n",
        "def expectation(X, theta):\n",
        "  pi =  theta[\"pi\"]\n",
        "  mus =  theta[\"mus\"]\n",
        "  covs =  theta[\"covs\"]\n",
        "  responsibilty = np.zeros((len(X),len(covs)))\n",
        "\n",
        "  for m,co,p,r in zip(mus, covs, pi,range(len(responsibilty[0]))):\n",
        "    co+=1e-6*np.identity(len(X[0]))\n",
        "    mn = multivariate_normal(mean=m,cov=co)\n",
        "    responsibilty[:,r] = p*mn.pdf(X)/np.sum([pi_c*multivariate_normal(mean=mu_c,cov=cov_c).pdf(X) for pi_c,mu_c,cov_c in zip(pi,mus,covs+epsilon)],axis=0)\n",
        "\n",
        "  return responsibilty\n",
        "\n",
        "############################################################################\n",
        "#Write a function for the maximization step. \n",
        "############################################################################\n",
        "\n",
        "def maximization(X, theta, theta_ubm, relevance, responsibility):\n",
        "  mus = []\n",
        "  covs = theta[\"covs\"]\n",
        "  pi = []\n",
        "  mu_ubm = theta_ubm[\"mus\"]\n",
        "  pi_ubm = theta_ubm[\"pi\"]\n",
        "\n",
        "  for c in range(len(responsibility[0])):\n",
        "    Nk = np.sum(responsibility[:,c],axis=0)\n",
        "    mu_em = (1/Nk)*np.sum(X*responsibility[:,c].reshape(len(X),1),axis=0)\n",
        "    pi_em = Nk/np.sum(responsibility)\n",
        "\n",
        "    # MAP Adaption Weighting\n",
        "    alpha = Nk/(Nk + relevance)\n",
        "    mu_spk = alpha*mu_em + (1-alpha)*mu_ubm[c]\n",
        "    pi_spk = alpha*pi_em + (1-alpha)*pi_ubm[c]\n",
        "\n",
        "    mus.append(mu_spk)\n",
        "    pi.append(pi_spk)\n",
        "\n",
        "  theta = {\"mus\" : np.array(mus), \"covs\" : covs, \"pi\" : np.array(pi)}\n",
        "  return theta\n",
        "\n",
        "############################################################################\n",
        "#Run the expectation and maximzation algorithm to estimate the parameters\n",
        "############################################################################\n",
        "\n",
        "def EMalgo(X, theta, theta_ubm, relevance, iters = 30):\n",
        "  logLikelihoods = np.zeros(iters)\n",
        "  thetas = []\n",
        "  for i in range(iters):\n",
        "    res = expectation(X, theta)\n",
        "    theta = maximization(X, theta, theta_ubm, relevance, res)\n",
        "    thetas.append(theta)\n",
        "    logLikelihoods[i] = logLikelihood(X, theta)\n",
        "  return thetas, logLikelihoods"
      ],
      "metadata": {
        "id": "bBM-Yc3QTGAN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GMM Training\n",
        "\n",
        "models = {}\n",
        "\n",
        "for speaker in speaker_feats.keys():\n",
        "  print(\"Training for Speaker \" + str(speaker))\n",
        "  X = speaker_wise_stacked[speaker]\n",
        "  theta = initialization(X, ubm_gm, 128)\n",
        "  theta_ubm = theta\n",
        "  thetas, logLikelihoods = EMalgo(X, theta, theta_ubm, relevance=0.9, iters=2)\n",
        "  gmm = {\n",
        "      \"theta\" : thetas,\n",
        "      \"loglikes\" : logLikelihoods\n",
        "  }\n",
        "  models[speaker] = gmm"
      ],
      "metadata": {
        "id": "pcXlEvdCb9ZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36416c3-49f6-4ba1-a8ae-60ef4b70a843"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for Speaker id10287\n",
            "Training for Speaker id10303\n",
            "Training for Speaker id10286\n",
            "Training for Speaker id10307\n",
            "Training for Speaker id10296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log Liklihood for single speaker with 10 EM Iterations\n",
        "plt.plot(models[\"id10287\"][\"loglikes\"])\n",
        "plt.title(\"Log Likelihood Plot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "__Qg-4-X6Wpk",
        "outputId": "cf845d26-e93b-4868-f1ae-8af9cab0ff14"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################\n",
        "# Computing the likelihood ratio\n",
        "############################################################################\n",
        "\n",
        "# Loading verification frames\n",
        "data_path = \"/content/ASV_Data/Verification_trials\"\n",
        "verify_df = pd.read_csv(data_path, sep=\" \", header=None, names=[\"label\", \"wav1\", \"wav2\"])\n",
        "\n",
        "for ind in verify_df.index:\n",
        "  verify_df[\"wav1\"][ind] = \"/content/ASV_Data/evaluation_data/\" + str(verify_df[\"wav1\"][ind])\n",
        "  verify_df[\"wav2\"][ind] = \"/content/ASV_Data/evaluation_data/\" + str(verify_df[\"wav2\"][ind])\n",
        "\n",
        "verify_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uqYZou4I454U",
        "outputId": "96539dc2-03af-4bfe-9f47-e784a721b736"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                           wav1                           wav2\n",
              "0      1  id10293/k1jiAINXHRI/00024.wav  id10293/Mp1IXRquJ9s/00005.wav\n",
              "1      1  id10296/ZEpT2orkZTc/00002.wav  id10296/DKDZ6ENKX0o/00002.wav\n",
              "2      0  id10283/vaK4t1-WD4M/00005.wav  id10304/GRv7pEnTwUc/00001.wav\n",
              "3      1  id10290/O-V_sInAw5M/00006.wav  id10290/T1y64qFjv3M/00003.wav\n",
              "4      1  id10293/TnOGBL8p-RM/00005.wav  id10293/TwfthltapLg/00004.wav"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a01420d6-e935-4d4d-800c-ffd48183a84e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>wav1</th>\n",
              "      <th>wav2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>id10293/k1jiAINXHRI/00024.wav</td>\n",
              "      <td>id10293/Mp1IXRquJ9s/00005.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>id10296/ZEpT2orkZTc/00002.wav</td>\n",
              "      <td>id10296/DKDZ6ENKX0o/00002.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>id10283/vaK4t1-WD4M/00005.wav</td>\n",
              "      <td>id10304/GRv7pEnTwUc/00001.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>id10290/O-V_sInAw5M/00006.wav</td>\n",
              "      <td>id10290/T1y64qFjv3M/00003.wav</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>id10293/TnOGBL8p-RM/00005.wav</td>\n",
              "      <td>id10293/TwfthltapLg/00004.wav</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a01420d6-e935-4d4d-800c-ffd48183a84e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a01420d6-e935-4d4d-800c-ffd48183a84e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a01420d6-e935-4d4d-800c-ffd48183a84e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in verify_df.keys():\n",
        "  likes = []\n",
        "  y, sr = librosa.load(df[\"filepath\"][ind], sr = None)\n",
        "  y = y/max(abs(y))\n",
        "  hop_length = int(sr*0.010)\n",
        "  frameSize = int(sr*0.025)\n",
        "  energies = librosa.feature.rms(y=y, frame_length = frameSize, hop_length = hop_length)\n",
        "  mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=20, win_length=frameSize)\n",
        "  indexes = energies[0] > 0.1\n",
        "  if len(indexes) == mfcc.shape[1]:\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "    feature = np.vstack([mfcc, mfcc_delta, mfcc_delta2])\n",
        "    feature = speechpy.processing.cmvnw(feature, win_size=301, variance_normalization=True)\n",
        "    X = feature[:,indexes].T\n",
        "  else:\n",
        "    break\n",
        "  for speaker in models.keys():\n",
        "    theta = models[speaker][\"theta\"]\n",
        "    likes.append(likelihood(X, theta))\n",
        "  likes = np.array(likes)\n",
        "  # pick the best model:\n",
        "  modelindex = np.where(likes == likes.min())\n",
        "  model = list(models.values())[modelindex]\n",
        "\n",
        "  # creating hypothesis ratio\n",
        "  "
      ],
      "metadata": {
        "id": "PoP3jDZF0oPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oofmh9SkaXZH"
      },
      "source": [
        "<h4> <b> Descriminative approaches to speaker verification (x-vector model) : </b>  This section describes the steps required to extract speaker embedding using the DNN framework. Please use the PyTorch toolkit for this assignment.\n",
        "\n",
        "<dt> <h4> <b> Feature Extraction </b> <dt> <h4>\n",
        "<dt> <h4> 1. Extract 64-dimensional log-Mel filter bank energies from the speech signal and use them as features to train the speaker classification network \n",
        "  </h4> </dt> \n",
        "\n",
        "<dt> <h4> <b> Architecture </b> <dt> <h4>\n",
        "<dt> <h4> 1. Frame Level Layers </h4> </dt> \n",
        "<dd> <h4> - Four layers of 1D CNN layers to extract high level speaker discriminative representations from the feature vectors.   </h4> </dd> \n",
        "<dt> <h4> 2. Stats Pooling Layer </h4> </dt> \n",
        "<dd> <h4> - Computes temporal mean and variance of framelevel representations to extract fixed dimensional embedding from the variable length representations.  </h4> </dd> \n",
        "<dt> <h4> 3. Segment level layers </h4> </dt> \n",
        "<dd> <h4> - A pair of fully connected layers and output layer as a classifier. The activations of first or second fully connected layers is considered as x-vector.</h4> </dd> \n",
        "<dt> <h4> <b> Dataset\n",
        "\n",
        "[link to data](https://drive.google.com/drive/folders/1Btnpm_QwSirInGKvL0yq5UL_zL23rM1f?usp=sharing)\n",
        "\n",
        "<dt> <h4> 1. Training data : Speech from 100 speakers is provided to train the speaker classification network. Training data is available in \"UBM_data\" folder.\n",
        "  </h4> </dt> \n",
        "  <dt> <h4> 2. Testing data : Speech from 40 speakers is provided to test the ASV system. \n",
        "  </h4> </dt> \n",
        "<dt> <h4> <b> Training </b> <dt> <h4>\n",
        "<dt> <h4> 1. Write a dataset loader to pass the features to the network  </h4> </dt> \n",
        "<dt> <h4> 2. As it is not possible to form the batch with variable length speech signal, typically, it is preferred to obtain a chunk ( 2 to 6 seconds ) of data from each utterance and form batches to train the network. Please follow the same for training the network.    </h4> </dt> \n",
        "<dt> <h4> 3. Compute cross entropy loss between true and predicted labels and use adam optimizer to train the network </h4> </dt> \n",
        "<dt> <h4> <b> Inference (Testing) </b> <dt> <h4>\n",
        "<dt> <h4> 1. Discard the logistic classification layer and extract speaker embeddings from the outputs of the dense layer in segment-level layers for both claimed identity and test utterance\n",
        " </h4> </dt> \n",
        "<dt> <h4> 2. Perform the cosine scoring between claimed identity ( enroll ) and test embeddings to compute the speaker similarity between them </h4> </dt> \n",
        "<dt> <h4> 3. Compute Equal Error Rate (EER) to quantify the performance of the ASV system. </h4> </dt> \n",
        "\n",
        "<dt> <h4> <b> Optional </b> <dt> <h4>\n",
        "\n",
        "<dt> <h4> 1. Replace the stats pooling layer with self-attentive stats pooling layer and observe the performance improvement. Self-attentinve pooling weights different frames based on their ability of representing the speaker information. </h4> </dt> \n",
        "<dt> <h4> 2. What did you observe from the self-attentive pooling experiments ? </h4> </dt> \n",
        "<dt> <h4> 3. Experiment with different frame-level encoders and report your observations. </h4> </dt> \n",
        "<dt> <h4> 4. Please report your observations on descrimatinve approaches to speaker verification </h4> </dt> \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "#################################\n",
        "#Dataset Manifests using Pandas\n",
        "#################################\n",
        "\n",
        "data_path='/content/ASV_Data/UBM_data'\n",
        "\n",
        "dataframe = []\n",
        "\n",
        "for subdir, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        #print os.path.join(subdir, file)\n",
        "        filepath = subdir + os.sep + file\n",
        "\n",
        "        if filepath.endswith(\".wav\"):\n",
        "\n",
        "            data = {\n",
        "                'filepath'  : filepath,\n",
        "                'speaker'   : os.path.basename(os.path.dirname(subdir))\n",
        "            }\n",
        "\n",
        "            dataframe.append(data)\n",
        "\n",
        "train_df = pd.DataFrame(dataframe)\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2CIoWtBtyTkj",
        "outputId": "faf4536d-12c8-4e6d-a0f6-9d8f81cb1a39"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            filepath  speaker\n",
              "0  /content/ASV_Data/UBM_data/id10611/GRdcuS1VOWo...  id10611\n",
              "1  /content/ASV_Data/UBM_data/id10611/GRdcuS1VOWo...  id10611\n",
              "2  /content/ASV_Data/UBM_data/id10611/GRdcuS1VOWo...  id10611\n",
              "3  /content/ASV_Data/UBM_data/id10611/7_ZlkxpOTsY...  id10611\n",
              "4  /content/ASV_Data/UBM_data/id10611/7_ZlkxpOTsY...  id10611"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-012bff6b-0a08-4f9e-b678-720c402eed9d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>speaker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10611/GRdcuS1VOWo...</td>\n",
              "      <td>id10611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10611/GRdcuS1VOWo...</td>\n",
              "      <td>id10611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10611/GRdcuS1VOWo...</td>\n",
              "      <td>id10611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10611/7_ZlkxpOTsY...</td>\n",
              "      <td>id10611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/ASV_Data/UBM_data/id10611/7_ZlkxpOTsY...</td>\n",
              "      <td>id10611</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-012bff6b-0a08-4f9e-b678-720c402eed9d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-012bff6b-0a08-4f9e-b678-720c402eed9d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-012bff6b-0a08-4f9e-b678-720c402eed9d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speakers = train_df.speaker.unique()\n",
        "mappings = dict(zip(speakers, range(len(speakers))))\n",
        "print(mappings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BIuyi5q2xFV",
        "outputId": "ab3b8d75-c174-438e-adc9-7fff72e2f9f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id10611': 0, 'id10436': 1, 'id10596': 2, 'id10087': 3, 'id10167': 4, 'id10503': 5, 'id10703': 6, 'id10320': 7, 'id10067': 8, 'id10607': 9, 'id10098': 10, 'id10412': 11, 'id10865': 12, 'id10062': 13, 'id10057': 14, 'id10386': 15, 'id10477': 16, 'id10586': 17, 'id10210': 18, 'id10882': 19, 'id10378': 20, 'id10234': 21, 'id10484': 22, 'id10189': 23, 'id10242': 24, 'id10650': 25, 'id10660': 26, 'id10721': 27, 'id10657': 28, 'id10769': 29, 'id10855': 30, 'id10479': 31, 'id10663': 32, 'id10444': 33, 'id10340': 34, 'id10797': 35, 'id10531': 36, 'id10129': 37, 'id10600': 38, 'id10843': 39, 'id10577': 40, 'id10212': 41, 'id10513': 42, 'id10536': 43, 'id10346': 44, 'id10644': 45, 'id10693': 46, 'id10125': 47, 'id10832': 48, 'id10144': 49, 'id10733': 50, 'id10448': 51, 'id10803': 52, 'id10201': 53, 'id10793': 54, 'id10517': 55, 'id10115': 56, 'id10136': 57, 'id10240': 58, 'id10039': 59, 'id10304': 60, 'id10509': 61, 'id10713': 62, 'id10228': 63, 'id10712': 64, 'id10403': 65, 'id10785': 66, 'id10156': 67, 'id10301': 68, 'id10679': 69, 'id10355': 70, 'id10764': 71, 'id10454': 72, 'id10018': 73, 'id10707': 74, 'id10208': 75, 'id10079': 76, 'id10211': 77, 'id10364': 78, 'id10842': 79, 'id10196': 80, 'id10452': 81, 'id10317': 82, 'id10026': 83, 'id10045': 84, 'id10626': 85, 'id10819': 86, 'id10861': 87, 'id10165': 88, 'id10433': 89, 'id10205': 90, 'id10699': 91, 'id10226': 92, 'id10718': 93, 'id10194': 94, 'id10227': 95, 'id10639': 96, 'id10520': 97, 'id10799': 98, 'id10662': 99}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_ids = mappings\n",
        "\n",
        "train_list = []\n",
        "\n",
        "for ind in tqdm(train_df.index):\n",
        "  line = train_df[\"filepath\"][ind] + \" \" + str(class_ids[train_df[\"speaker\"][ind]])\n",
        "  train_list.append(line)\n",
        "\n",
        "def create_meta(files_list,store_loc,mode='train'):\n",
        "    if not os.path.exists(store_loc):\n",
        "        os.makedirs(store_loc)\n",
        "    \n",
        "    if mode=='train':\n",
        "        meta_store = store_loc+'/training.txt'\n",
        "        fid = open(meta_store,'w')\n",
        "        for filepath in files_list:\n",
        "            fid.write(filepath+'\\n')\n",
        "        fid.close()\n",
        "    else:\n",
        "        print('Error in creating meta files')\n",
        "    \n",
        "create_meta(train_list,'/content/',mode='train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I4MEzxI3OFi",
        "outputId": "521528ff-8077-4cd8-9a5b-e424c62b65b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11667/11667 [00:00<00:00, 91770.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YlJJK_YddCNY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#####################################################\n",
        "#Network Architecture\n",
        "#####################################################\n",
        "\n",
        "class TDNN(nn.Module):\n",
        "    \n",
        "    def __init__(\n",
        "                    self, \n",
        "                    input_dim=23, \n",
        "                    output_dim=512,\n",
        "                    context_size=5,\n",
        "                    stride=1,\n",
        "                    dilation=1,\n",
        "                    batch_norm=False,\n",
        "                    dropout_p=0.2\n",
        "                ):\n",
        "        super(TDNN, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.stride = stride\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dilation = dilation\n",
        "        self.dropout_p = dropout_p\n",
        "        self.batch_norm = batch_norm\n",
        "      \n",
        "        self.kernel = nn.Linear(input_dim*context_size, output_dim)\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "        if self.batch_norm:\n",
        "            self.bn = nn.BatchNorm1d(output_dim)\n",
        "        if self.dropout_p:\n",
        "            self.drop = nn.Dropout(p=self.dropout_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        _, _, d = x.shape\n",
        "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Unfold input into smaller temporal contexts\n",
        "        x = F.unfold(\n",
        "                        x, \n",
        "                        (self.context_size, self.input_dim), \n",
        "                        stride=(1,self.input_dim), \n",
        "                        dilation=(self.dilation,1)\n",
        "                    )\n",
        "\n",
        "        # N, output_dim*context_size, new_t = x.shape\n",
        "        x = x.transpose(1,2)\n",
        "        x = self.kernel(x.float())\n",
        "        x = self.nonlinearity(x)\n",
        "        \n",
        "        if self.dropout_p:\n",
        "            x = self.drop(x)\n",
        "\n",
        "        if self.batch_norm:\n",
        "            x = x.transpose(1,2)\n",
        "            x = self.bn(x)\n",
        "            x = x.transpose(1,2)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Xvector(nn.Module):\n",
        "    def __init__(self, input_dim = 40, num_classes=100):\n",
        "        super(Xvector, self).__init__()\n",
        "        self.tdnn1 = TDNN(input_dim=input_dim, output_dim=512, context_size=5, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn2 = TDNN(input_dim=512, output_dim=512, context_size=3, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn3 = TDNN(input_dim=512, output_dim=512, context_size=2, dilation=2,dropout_p=0.5)\n",
        "        self.tdnn4 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=1,dropout_p=0.5)\n",
        "        self.tdnn5 = TDNN(input_dim=512, output_dim=512, context_size=1, dilation=3,dropout_p=0.5)\n",
        "        #### Frame levelPooling\n",
        "        self.segment6 = nn.Linear(1024, 512)\n",
        "        self.segment7 = nn.Linear(512, 512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        tdnn1_out = self.tdnn1(inputs)\n",
        "        tdnn2_out = self.tdnn2(tdnn1_out)\n",
        "        tdnn3_out = self.tdnn3(tdnn2_out)\n",
        "        tdnn4_out = self.tdnn4(tdnn3_out)\n",
        "        tdnn5_out = self.tdnn5(tdnn4_out)\n",
        "        ### Stat Pool\n",
        "        \n",
        "        mean = torch.mean(tdnn5_out,1)\n",
        "        std = torch.var(tdnn5_out,1)\n",
        "        stat_pooling = torch.cat((mean,std),1)\n",
        "        segment6_out = self.segment6(stat_pooling)\n",
        "        x_vec = self.segment7(segment6_out)\n",
        "        predictions = self.output(x_vec)\n",
        "        return predictions,x_vec\n",
        "\n",
        "#######################################\n",
        "#Define cross entropy loss function\n",
        "#######################################\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#######################################\n",
        "#Write data set loader\n",
        "#######################################\n",
        "\n",
        "def load_audio(link, min_duration, mode='train'):\n",
        "    audio_data, sr = librosa.load(link, sr=16000)\n",
        "    len_file = len(audio_data)\n",
        "    if len_file <int(min_duration*sr):\n",
        "        dummy=np.zeros((1,int(min_duration*sr)-len_file))\n",
        "        extened_wav = np.concatenate((audio_data,dummy[0]))\n",
        "    else:\n",
        "        extened_wav = audio_data\n",
        "\n",
        "    win_length=400\n",
        "    hop_length=160\n",
        "    n_mels=40\n",
        "    spec_len=400\n",
        "    mode='train'\n",
        "    n_fft=512\n",
        "\n",
        "    linear_spect = librosa.stft(extened_wav, n_fft=n_fft, win_length=win_length, hop_length=hop_length).T\n",
        "    mag, _ = librosa.magphase(linear_spect)  # magnitude\n",
        "    mag_T = mag.T\n",
        "    \n",
        "    if mode=='train':\n",
        "        randtime = np.random.randint(0, mag_T.shape[1]-spec_len)\n",
        "        spec_mag = mag_T[:, randtime:randtime+spec_len]\n",
        "    else:\n",
        "        spec_mag = mag_T\n",
        "    \n",
        "    # preprocessing, subtract mean, divided by time-wise var\n",
        "    mu = np.mean(spec_mag, 0, keepdims=True)\n",
        "    std = np.std(spec_mag, 0, keepdims=True)\n",
        "    return (spec_mag - mu) / (std + 1e-5)\n",
        "\n",
        "def speech_collate(batch):\n",
        "    targets = []\n",
        "    specs = []\n",
        "    for sample in batch:\n",
        "        specs.append(sample['features'])\n",
        "        targets.append((sample['labels']))\n",
        "    return specs, targets\n",
        "\n",
        "class SpeakerDataset(Dataset):\n",
        "    \"\"\"Speech dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, manifest, mode):\n",
        "        \"\"\"\n",
        "        Read the textfile and get the paths\n",
        "        \"\"\"\n",
        "        self.mode=mode\n",
        "        self.audio_links = [line.rstrip('\\n').split(' ')[0] for line in open(manifest)]\n",
        "        self.labels = [int(line.rstrip('\\n').split(' ')[1]) for line in open(manifest)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_links)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_link =self.audio_links[idx]\n",
        "        class_id = self.labels[idx]\n",
        "        spec = load_audio(audio_link, min_duration = 4, mode=self.mode)\n",
        "        sample = {'features': torch.from_numpy(np.ascontiguousarray(spec)), 'labels': torch.from_numpy(np.ascontiguousarray(class_id))}\n",
        "        return sample\n",
        "\n",
        "training_filepath = \"/content/training.txt\"\n",
        "dataset_train = SpeakerDataset(manifest=training_filepath, mode='train')\n",
        "train_iter = DataLoader(dataset_train, batch_size=256, shuffle=True,collate_fn=speech_collate) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "#Train the network\n",
        "#######################################\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Xvector(input_dim=257, num_classes=len(class_ids.keys())).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "def train(model, loss_fn, train_loader, epochs, optimizer):\n",
        "  for epoch in tqdm(range(1,epochs+1)):\n",
        "    model.train()\n",
        "    full_preds=[]\n",
        "    full_gts=[]\n",
        "    train_loss_list=[]\n",
        "\n",
        "     # forward and backward propagations with train data\n",
        "    for i, data in enumerate(train_loader):\n",
        "        features = torch.from_numpy(np.asarray([torch_tensor.numpy().T for torch_tensor in data[0]])).float()\n",
        "        labels = torch.from_numpy(np.asarray([torch_tensor[0].numpy() for torch_tensor in data[1]]))\n",
        "        features, labels = features.to(device),labels.to(device)\n",
        "        features.requires_grad = True\n",
        "        optimizer.zero_grad()\n",
        "        y_hat,x_vec = model(features)        \n",
        "        loss = loss_fn(y_hat, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_list.append(loss.item())\n",
        "\n",
        "    predictions = np.argmax(y_hat.detach().cpu().numpy(),axis=1)\n",
        "    for pred in predictions:\n",
        "        full_preds.append(pred)\n",
        "    for lab in labels.detach().cpu().numpy():\n",
        "        full_gts.append(lab)\n",
        "    \n",
        "    mean_acc = accuracy_score(full_gts,full_preds)\n",
        "    mean_loss = np.mean(np.asarray(train_loss_list))\n",
        "    print('Total training loss {} and training Accuracy {} after {} epochs'.format(mean_loss,mean_acc,epoch))\n",
        "\n",
        "loss_fn = criterion\n",
        "EPOCHS = 7\n",
        "train(model, loss_fn, train_iter, EPOCHS, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK-6DVPe-Y7z",
        "outputId": "2cbfd3db-9854-489e-bbaf-66f9e1e80d51"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 1/7 [04:40<28:05, 281.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 4.4606867458509365 and training Accuracy 0.05442176870748299 after 1 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▊       | 2/7 [09:15<23:05, 277.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 4.008342431939167 and training Accuracy 0.11564625850340136 after 2 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 3/7 [14:13<19:05, 286.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 3.704968079276707 and training Accuracy 0.08843537414965986 after 3 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 4/7 [19:14<14:36, 292.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 3.5894797262938125 and training Accuracy 0.10884353741496598 after 4 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████▏  | 5/7 [24:03<09:42, 291.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 3.4567816516627436 and training Accuracy 0.12244897959183673 after 5 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 6/7 [29:04<04:54, 294.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 3.3192365739656533 and training Accuracy 0.23809523809523808 after 6 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [34:09<00:00, 292.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training loss 3.144595203192338 and training Accuracy 0.25170068027210885 after 7 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = {'model': model.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "torch.save(state_dict, '/content/saved_model')"
      ],
      "metadata": {
        "id": "-lsLsDD4drH9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up verification dataframe\n",
        "\n",
        "data_path = \"/content/ASV_Data/Verification_trials\"\n",
        "verify_df = pd.read_csv(data_path, sep=\" \", header=None, names=[\"label\", \"wav1\", \"wav2\"])\n",
        "\n",
        "for ind in verify_df.index:\n",
        "  verify_df[\"wav1\"][ind] = \"/content/ASV_Data/evaluation_data/\" + str(verify_df[\"wav1\"][ind])\n",
        "  verify_df[\"wav2\"][ind] = \"/content/ASV_Data/evaluation_data/\" + str(verify_df[\"wav2\"][ind])\n",
        "\n",
        "data_path = \"/content/ASV_Data/test_files\"\n",
        "test_df = pd.read_csv(data_path, sep=\" \", header=None, names=[\"filepath\"])\n",
        "\n",
        "labels = []\n",
        "for ind in test_df.index:\n",
        "  labels.append(test_df[\"filepath\"][ind][:7])\n",
        "  test_df[\"filepath\"][ind] = \"/content/ASV_Data/evaluation_data/\" + str(test_df[\"filepath\"][ind])\n",
        "\n",
        "test_df[\"label\"] = labels\n",
        "\n",
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "S7UXekRLeG1o",
        "outputId": "c10ee5a1-94a7-46af-9417-458d4ee1f862"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            filepath    label\n",
              "0  /content/ASV_Data/evaluation_data/id10270/5r0d...  id10270\n",
              "1  /content/ASV_Data/evaluation_data/id10270/5r0d...  id10270\n",
              "2  /content/ASV_Data/evaluation_data/id10270/5r0d...  id10270\n",
              "3  /content/ASV_Data/evaluation_data/id10270/5r0d...  id10270\n",
              "4  /content/ASV_Data/evaluation_data/id10270/5r0d...  id10270"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49e42da6-6d1f-4807-8287-59a86f8adfc9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/ASV_Data/evaluation_data/id10270/5r0d...</td>\n",
              "      <td>id10270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/ASV_Data/evaluation_data/id10270/5r0d...</td>\n",
              "      <td>id10270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/ASV_Data/evaluation_data/id10270/5r0d...</td>\n",
              "      <td>id10270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/ASV_Data/evaluation_data/id10270/5r0d...</td>\n",
              "      <td>id10270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/ASV_Data/evaluation_data/id10270/5r0d...</td>\n",
              "      <td>id10270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49e42da6-6d1f-4807-8287-59a86f8adfc9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49e42da6-6d1f-4807-8287-59a86f8adfc9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49e42da6-6d1f-4807-8287-59a86f8adfc9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "#Inference from the network \n",
        "#######################################\n",
        "\n",
        "pred = []\n",
        "model.eval()\n",
        "\n",
        "for ind in verify_df.index:\n",
        "  with torch.no_grad():\n",
        "\n",
        "#######################################\n",
        "#Evaluate the performance i.e Report the EER\n",
        "#######################################\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iHAx2K-VZEZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GbIc6QRvnin"
      },
      "source": [
        "<h4> <b> Report of the assignment : </b>  \n",
        "Write down your observations \n",
        "<dt> <h4> 1.      </h4> </dt>\n",
        "<dt> <h4> 2.      </h4> </dt>\n",
        "<dt> <h4> 3.      </h4> </dt>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}